default:
  lm: 'LLaMA' # ['RoBERTa', 'LLaMA']
  seed: [0, 1, 2, 3, 4]
  epochs: 10
  valid_epoch: 1
  warmup_epochs: 1
  lr: 2e-4
  min_lr: 5e-6
  grad_steps: 2
  weight_decay: 5e-2
  dropout: 0.1
  att_dropout: 0.1
  patience: 3
  batch_size: 2
  max_length: 256

  LoRA:
    use_lora: True
    lora_r: 5
    lora_alpha: 16
    lora_dropout: 0.05

  gp_gnn: 'GCN'
  gp_layer_num: 4
  gp_hidden_dim: 1024
  gp_output_dim: 1024
  gp_dropout: 0.5
  gp_num_heads: 4 # only available for GATConv
  gp_aggr: 'mean' # ['mean', 'max', 'lstm'], only available for SAGEConv
  gp_proj_hidden_dim: 1024
search_space:
  lr: [1e-4, 1e-3]
  gp_gnn: ['GCN', 'SAGE']
  gp_layer_num: [2, 4]